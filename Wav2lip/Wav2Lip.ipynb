{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pia-fml1/DeepFake-Voice-Cloning/blob/main/Wav2lip/Wav2Lip.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "OZJJ-zPGiiIs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0b9030a7-36b0-4b30-9a52-e2c35ec21516"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "checking for GPU\n",
            "requesting Google Drive access\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "fatal: destination path 'DeepFake-Voice-Cloning' already exists and is not an empty directory.\n",
            "/content/DeepFake-Voice-Cloning/Wav2lip\n",
            "Cloning into 'face-alignment'...\n",
            "remote: Enumerating objects: 1122, done.\u001b[K\n",
            "remote: Counting objects: 100% (153/153), done.\u001b[K\n",
            "remote: Compressing objects: 100% (90/90), done.\u001b[K\n",
            "remote: Total 1122 (delta 79), reused 108 (delta 56), pack-reused 969 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1122/1122), 6.28 MiB | 11.82 MiB/s, done.\n",
            "Resolving deltas: 100% (680/680), done.\n",
            "installing batch_face\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: torch==2.1.0+cu121 in /usr/local/lib/python3.11/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision==0.16.0+cu121 in /usr/local/lib/python3.11/dist-packages (0.16.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0+cu121) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0+cu121) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0+cu121) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0+cu121) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0+cu121) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0+cu121) (2024.10.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0+cu121) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.16.0+cu121) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchvision==0.16.0+cu121) (2.32.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.16.0+cu121) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.1.0+cu121) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.0+cu121) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.0+cu121) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.0+cu121) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.0+cu121) (2024.12.14)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.1.0+cu121) (1.3.0)\n",
            "installing gfpgan\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for basicsr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for filterpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading wav2lip essentials\n",
            "Downloading: \"https://github.com/anothermartz/Easy-Wav2Lip/releases/download/Prerequesits/Wav2Lip_GAN.pth\" to /content/DeepFake-Voice-Cloning/Wav2lip/checkpoints/Wav2Lip_GAN.pth\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 416M/416M [00:04<00:00, 100MB/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading /content/DeepFake-Voice-Cloning/Wav2lip/checkpoints/Wav2Lip_GAN.pth\n",
            "Wav2Lip_GAN loaded\n",
            "Downloading: \"https://github.com/anothermartz/Easy-Wav2Lip/releases/download/Prerequesits/Wav2Lip.pth\" to /content/DeepFake-Voice-Cloning/Wav2lip/checkpoints/Wav2Lip.pth\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 416M/416M [00:08<00:00, 53.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading /content/DeepFake-Voice-Cloning/Wav2lip/checkpoints/Wav2Lip.pth\n",
            "wav2lip loaded\n",
            "downloading gfpgan essentials\n",
            "Downloading: \"https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.4.pth\" to /content/DeepFake-Voice-Cloning/Wav2lip/checkpoints/GFPGANv1.4.pth\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 332M/332M [00:02<00:00, 120MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://github.com/xinntao/facexlib/releases/download/v0.1.0/detection_Resnet50_Final.pth\" to /content/DeepFake-Voice-Cloning/Wav2lip/gfpgan/weights/detection_Resnet50_Final.pth\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104M/104M [00:00<00:00, 304MB/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://github.com/xinntao/facexlib/releases/download/v0.2.2/parsing_parsenet.pth\" to /content/DeepFake-Voice-Cloning/Wav2lip/gfpgan/weights/parsing_parsenet.pth\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81.4M/81.4M [00:00<00:00, 210MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initializing face detectors\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Unable to open checkpoints/shape_predictor_68_face_landmarks_GTX.dat",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-fb1f6730185a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'initializing face detectors'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;31m#load face detectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m \u001b[0mload_predictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;31m#write a file to signify setup is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/DeepFake-Voice-Cloning/Wav2lip/easy_functions.py\u001b[0m in \u001b[0;36mload_predictor\u001b[0;34m()\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;34m\"checkpoints\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"shape_predictor_68_face_landmarks_GTX.dat\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     )\n\u001b[0;32m--> 144\u001b[0;31m     \u001b[0mpredictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape_predictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m     \u001b[0mmouth_detector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_frontal_face_detector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Unable to open checkpoints/shape_predictor_68_face_landmarks_GTX.dat"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "if os.path.exists('installed.txt'):\n",
        "  sys.exit('Step 1 has already been run on this instance!')\n",
        "\n",
        "print('checking for GPU')\n",
        "#check GPU is enabled\n",
        "import torch\n",
        "if not torch.cuda.is_available():\n",
        "  sys.exit('No GPU in runtime. Please go to the \"Runtime\" menu, \"Change runtime type\" and select \"GPU\".')\n",
        "\n",
        "print('requesting Google Drive access')\n",
        "#prompt to mount google drive\n",
        "try:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "except:\n",
        "  print(\"google drive not linked\")\n",
        "\n",
        "#start timer\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "#clone git\n",
        "giturl = 'https://github.com/pia-fml1/DeepFake-Voice-Cloning.git'\n",
        "gitbranch = 'main'\n",
        "!git clone -b {gitbranch} {giturl}\n",
        "import re\n",
        "regex = r'([^\\/]+)(?=\\.git)'\n",
        "match = re.search(regex, giturl)\n",
        "project_dir = match.group(1)+'/Wav2lip'\n",
        "%cd '{project_dir}'\n",
        "!mkdir 'face_alignment' 'temp'\n",
        "\n",
        "#get face_alignment folder\n",
        "!git clone https://github.com/1adrianb/face-alignment.git\n",
        "!mv face-alignment/face_alignment/* face_alignment/\n",
        "!rm -rf face-alignment\n",
        "\n",
        "#install prerequisites\n",
        "print('installing batch_face')\n",
        "!pip install torch==2.1.0+cu121 torchvision==0.16.0+cu121 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install batch_face --quiet\n",
        "print('installing gfpgan')\n",
        "!pip install gfpgan --quiet\n",
        "\n",
        "#import functions\n",
        "from easy_functions import (format_time,\n",
        "                            get_input_length,\n",
        "                            get_video_details,\n",
        "                            load_file_from_url,\n",
        "                            load_model,\n",
        "                            load_predictor,\n",
        "                            show_video)\n",
        "import contextlib\n",
        "import face_alignment\n",
        "import shutil\n",
        "import subprocess\n",
        "import warnings\n",
        "from enhance import load_sr\n",
        "from IPython.display import Audio, Image, clear_output, display\n",
        "from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip\n",
        "\n",
        "#download and initialize both wav2lip models\n",
        "print('downloading wav2lip essentials')\n",
        "load_file_from_url(\n",
        "  url='https://github.com/anothermartz/Easy-Wav2Lip/releases/download/Prerequesits/Wav2Lip_GAN.pth',\n",
        "  model_dir='checkpoints', progress=True, file_name='Wav2Lip_GAN.pth')\n",
        "model = load_model(\"/content/\"+project_dir+\"/checkpoints/Wav2Lip_GAN.pth\")\n",
        "print('Wav2Lip_GAN loaded')\n",
        "load_file_from_url(\n",
        "  url='https://github.com/anothermartz/Easy-Wav2Lip/releases/download/Prerequesits/Wav2Lip.pth',\n",
        "  model_dir='checkpoints', progress=True, file_name='Wav2Lip.pth')\n",
        "model = load_model(\"/content/\"+project_dir+\"/checkpoints/Wav2Lip.pth\")\n",
        "print('wav2lip loaded')\n",
        "\n",
        "#download gfpgan files\n",
        "print(\"downloading gfpgan essentials\")\n",
        "load_file_from_url(\n",
        "  url='https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.4.pth',\n",
        "  model_dir='checkpoints', progress=True, file_name='GFPGANv1.4.pth')\n",
        "load_sr()\n",
        "\n",
        "print('initializing face detectors')\n",
        "#load face detectors\n",
        "load_predictor()\n",
        "\n",
        "#write a file to signify setup is done\n",
        "with open('installed.txt', 'w') as f:\n",
        "    f.write('Wav2Lip has been installed.')\n",
        "clear_output()\n",
        "print(\"Installation complete, move to Step 2!\")\n",
        "#end timer\n",
        "elapsed_time = time.time() - start_time\n",
        "print(f\"Execution time: {format_time(elapsed_time)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KS7vOhVWhgR7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "if not os.path.exists('installed.txt'):\n",
        "  sys.exit('Step 1 has not been run in this instance! Please run step 1 each time you disconnect from a runtime.')\n",
        "start_time = time.time()\n",
        "############################## user inputs #####################################\n",
        "video_file = \"\"\n",
        "vocal_file = \"\"\n",
        "\n",
        "if not os.path.exists(video_file):\n",
        "  sys.exit('Video file not found!')\n",
        "\n",
        "quality = \"Enhanced\" #[\"Fast\", \"Improved\", \"Enhanced\", \"Experimental\"]\n",
        "#preview_quality = False #@param {type:\"boolean\"} - coming soon!\n",
        "output_height = \"full resolution\"\n",
        "if quality == \"Fast\":\n",
        "  no_sr=True\n",
        "  better_mask=False\n",
        "if quality == \"Improved\":\n",
        "  no_sr=True\n",
        "  better_mask=True\n",
        "if quality == \"Enhanced\":\n",
        "  no_sr=False\n",
        "  better_mask=True\n",
        "\n",
        "delete_previous_track = False\n",
        "wav2lip_version = \"Wav2Lip\"\n",
        "if wav2lip_version==\"Wav2Lip_GAN\":\n",
        "  checkpoint_path = '/content/'+project_dir+'/checkpoints/Wav2Lip_GAN.pth'\n",
        "else:\n",
        "  checkpoint_path = '/content/'+project_dir+'/checkpoints/Wav2Lip.pth'\n",
        "\n",
        "nosmooth = True\n",
        "U = 0\n",
        "D = 0\n",
        "L = 0\n",
        "R = 0\n",
        "\n",
        "size = 2.5\n",
        "feathering = 2\n",
        "mouth_tracking = False\n",
        "debug_mask = False\n",
        "\n",
        "if feathering == 3:\n",
        "  feathering = 5\n",
        "if feathering == 2:\n",
        "  feathering = 3\n",
        "\n",
        "resolution_scale = 1\n",
        "res_custom = False\n",
        "if output_height == 'half resolution':\n",
        "  resolution_scale = 2\n",
        "elif output_height == 'full resolution':\n",
        "  resolution_scale = 1\n",
        "else:\n",
        "  res_custom = True\n",
        "  resolution_scale = 3\n",
        "\n",
        "in_width, in_height, in_fps, in_length = get_video_details(video_file)\n",
        "out_height = round(in_height / resolution_scale)\n",
        "\n",
        "if res_custom:\n",
        "  out_height = int(output_height)\n",
        "fps_for_static_image = 30\n",
        "batch_process = False\n",
        "output_suffix = \"_EZWav2Lip\"\n",
        "include_settings_in_suffix = False\n",
        "\n",
        "if output_suffix == '' and not include_settings_in_suffix:\n",
        "  sys.exit('Current suffix settings will overwrite your input video! Please add a suffix or tick include_settings_in_suffix')\n",
        "\n",
        "preview_input = False\n",
        "preview_settings = False\n",
        "frame_to_preview = 100\n",
        "frame_to_preview = max(frame_to_preview -1,0)\n",
        "\n",
        "if include_settings_in_suffix:\n",
        "  if wav2lip_version==\"Wav2Lip_GAN\":\n",
        "    output_suffix = f'{output_suffix}_GAN'\n",
        "  output_suffix = f'{output_suffix}_{quality}'\n",
        "  if output_height != 'full resolution':\n",
        "    output_suffix = f'{output_suffix}_{out_height}'\n",
        "  if nosmooth:\n",
        "    output_suffix = f'{output_suffix}_nosmooth1'\n",
        "  else:\n",
        "    output_suffix = f'{output_suffix}_nosmooth0'\n",
        "  if U!=0 or D!=0 or L!=0 or R!=0:\n",
        "    output_suffix = f'{output_suffix}_pads-'\n",
        "    if U!=0:\n",
        "      output_suffix = f'{output_suffix}U{U}'\n",
        "    if D!=0:\n",
        "      output_suffix = f'{output_suffix}D{D}'\n",
        "    if L!=0:\n",
        "      output_suffix = f'{output_suffix}L{L}'\n",
        "    if R!=0:\n",
        "      output_suffix = f'{output_suffix}R{R}'\n",
        "  if quality != 'fast':\n",
        "    output_suffix = f'{output_suffix}_mask-S{size}F{feathering}'\n",
        "    if mouth_tracking:\n",
        "      output_suffix = f'{output_suffix}_mt'\n",
        "    if debug_mask:\n",
        "      output_suffix = f'{output_suffix}_debug'\n",
        "if preview_settings:\n",
        "  output_suffix = f'{output_suffix}_preview'\n",
        "\n",
        "\n",
        "rescaleFactor = str(round(1 // resolution_scale))\n",
        "pad_up = str(round(U * resolution_scale))\n",
        "pad_down = str(round(D * resolution_scale))\n",
        "pad_left = str(round(L * resolution_scale))\n",
        "pad_right = str(round(R * resolution_scale))\n",
        "################################################################################\n",
        "\n",
        "\n",
        "######################### reconstruct input paths ##############################\n",
        "# check video_file exists\n",
        "if not os.path.exists(video_file):\n",
        "  sys.exit(f'Could not find file: {video_file}')\n",
        "# extract each part of the path\n",
        "filename = re.search(r\"[^\\/]+(?=\\.\\w+$)\", video_file).group()\n",
        "file_type = os.path.splitext(video_file)[1]\n",
        "folder = re.search(r\"^(.*\\/)[^\\/]+$\", video_file).group(1)\n",
        "filenumber_match = re.search(r\"\\d+$\", filename)\n",
        "if filenumber_match: # if there is a filenumber - extract it\n",
        "  filenumber = str(filenumber_match.group())\n",
        "  filenamenonumber = re.sub(r\"\\d+$\", \"\", filename)\n",
        "else: # if there is no filenumber - make it blank\n",
        "  filenumber = \"\"\n",
        "  filenamenonumber = filename\n",
        "\n",
        "# if vocal_file is blank - use the video as audio\n",
        "if vocal_file == \"\":\n",
        "  vocal_file = video_file\n",
        "# if not, check that the vocal_file file exists\n",
        "else:\n",
        "  if not os.path.exists(vocal_file):\n",
        "    sys.exit(f'Could not find file: {vocal_file}')\n",
        "# extract each part of the path:\n",
        "audio_filename = re.search(r\"[^\\/]+(?=\\.\\w+$)\", vocal_file).group()\n",
        "audio_file_type = os.path.splitext(vocal_file)[1]\n",
        "audio_folder = re.search(r\"^(.*\\/)[^\\/]+$\", vocal_file).group(1)\n",
        "audio_filenumber_match = re.search(r\"\\d+$\", audio_filename)\n",
        "if audio_filenumber_match: #if there is a filenumber - extract it\n",
        "  audio_filenumber = str(audio_filenumber_match.group())\n",
        "  audio_filenamenonumber = re.sub(r\"\\d+$\", \"\", audio_filename)\n",
        "else: # if there is no filenumber - make it blank\n",
        "  audio_filenumber = \"\"\n",
        "  audio_filenamenonumber = audio_filename\n",
        "################################################################################\n",
        "\n",
        "# set process_failed to False so that it may be set to True if one or more processings fail\n",
        "process_failed = False\n",
        "temp_output = '/content/'+project_dir+'/temp/output.mp4'\n",
        "temp_folder = '/content/'+project_dir+'/temp/'\n",
        "last_input_video = None\n",
        "last_input_audio = None\n",
        "\n",
        "#--------------------------Batch processing loop-------------------------------!\n",
        "while True:\n",
        "\n",
        "  # construct input_video\n",
        "\n",
        "  input_video = folder + filenamenonumber + str(filenumber) + file_type\n",
        "  input_videofile = re.search(r\"[^\\/]+$\", input_video).group()\n",
        "  # construct input_audio\n",
        "  input_audio = audio_folder + audio_filenamenonumber + str(audio_filenumber) + audio_file_type\n",
        "  input_audiofile = re.search(r\"[^\\/]+$\", input_audio).group()\n",
        "  # see if filenames are different:\n",
        "  if filenamenonumber + str(filenumber) != audio_filenamenonumber + str(audio_filenumber):\n",
        "    output_filename = filenamenonumber + str(filenumber) + \"_\" + audio_filenamenonumber + str(audio_filenumber)\n",
        "  else:\n",
        "    output_filename = filenamenonumber + str(filenumber)\n",
        "  # construct output_video\n",
        "  output_video = folder + output_filename + output_suffix + '.mp4'\n",
        "  output_videofile = re.search(r\"[^\\/]+$\", output_video).group()\n",
        "\n",
        "  # remove last outputs\n",
        "  !rm -rf temp\n",
        "  !mkdir 'temp'\n",
        "\n",
        "  # preview inputs (if enabled)\n",
        "  if preview_input:\n",
        "    print(\"input video:\")\n",
        "    show_video(input_video)\n",
        "    if vocal_file != \"\":\n",
        "      print(\"input audio:\")\n",
        "      display(Audio(input_audio))\n",
        "    else:\n",
        "      print(\"using\", input_videofile, \"for audio\")\n",
        "    print(\"You may want to check now that they're the correct files!\")\n",
        "\n",
        "  last_input_video = input_video\n",
        "  last_input_audio = input_audio\n",
        "  shutil.copy(input_video, temp_folder)\n",
        "  shutil.copy(input_audio, temp_folder)\n",
        "\n",
        "  #rename temp file to include padding or else changing padding does nothing\n",
        "  temp_input_video = temp_folder + input_videofile\n",
        "  renamed_temp_input_video = temp_folder + str(U)+str(D)+str(L)+str(R) + input_videofile\n",
        "  shutil.copy(temp_input_video, renamed_temp_input_video)\n",
        "  temp_input_video = renamed_temp_input_video\n",
        "  temp_input_videofile = re.search(r\"[^\\/]+$\", temp_input_video).group()\n",
        "  temp_input_audio = temp_folder + input_audiofile\n",
        "\n",
        "    #trim video if it's longer than the audio\n",
        "  video_length = get_input_length(temp_input_video)\n",
        "  audio_length = get_input_length(temp_input_audio)\n",
        "\n",
        "  if preview_settings:\n",
        "    batch_process = False\n",
        "\n",
        "    preview_length_seconds = 1\n",
        "    converted_preview_frame = frame_to_preview/in_fps\n",
        "    preview_start_time = min(converted_preview_frame, video_length-preview_length_seconds)\n",
        "\n",
        "    preview_video_path = \"temp/preview_\" +str(preview_start_time)+'_' + str(U)+str(D)+str(L)+str(R) + input_videofile\n",
        "    preview_audio_path = \"temp/preview_\" + input_audiofile\n",
        "\n",
        "    if os.path.isfile(preview_video_path):\n",
        "      os.remove(preview_video_path)\n",
        "\n",
        "    subprocess.call(['ffmpeg', '-i', temp_input_video, '-ss', str(preview_start_time), '-to', str(preview_start_time+preview_length_seconds), '-c', 'copy', preview_video_path])\n",
        "    subprocess.call(['ffmpeg', '-i', temp_input_audio, '-ss', str(preview_start_time), '-to', str(preview_start_time+1), '-c', 'copy', preview_audio_path])\n",
        "    temp_input_video = preview_video_path\n",
        "    temp_input_audio = preview_audio_path\n",
        "\n",
        "  if video_length > audio_length:\n",
        "\n",
        "    trimmed_video_path = \"temp/trimmed_\" + temp_input_videofile\n",
        "    if os.path.isfile(trimmed_video_path):\n",
        "      os.remove(trimmed_video_path)\n",
        "    with open(os.devnull, 'w') as devnull:\n",
        "      with contextlib.redirect_stdout(devnull), contextlib.redirect_stderr(devnull):\n",
        "        ffmpeg_extract_subclip(temp_input_video, 0, audio_length, targetname=trimmed_video_path)\n",
        "    temp_input_video = trimmed_video_path\n",
        "\n",
        "  #check if face detection has already happened on this clip\n",
        "  last_detected_face = '/content/'+project_dir+'/face_alignment/last_detected_face.pkl'\n",
        "  if os.path.isfile('last_file.txt'):\n",
        "    with open('last_file.txt', 'r') as file:\n",
        "      last_file = file.readline()\n",
        "    if last_file != temp_input_video or delete_previous_track:\n",
        "        if os.path.isfile(last_detected_face):\n",
        "          os.remove(last_detected_face)\n",
        "\n",
        "  if os.path.isfile(temp_output):\n",
        "    os.remove(temp_output)\n",
        "\n",
        "  #----------------------------Process the inputs!-----------------------------!\n",
        "  print(f\"Processing{' preview:' if preview_settings else ''} {input_videofile} using {input_audiofile} for audio\")\n",
        "  #start processing timer\n",
        "  #start_time = time.time()\n",
        "\n",
        "\n",
        "  #execute Wav2Lip & upscaler\n",
        "  !python 'inference.py' \\\n",
        "  --face \"{temp_input_video}\" \\\n",
        "  --audio \"{temp_input_audio}\" \\\n",
        "  --outfile \"{temp_output}\" \\\n",
        "  --pads {pad_up} {pad_down} {pad_left} {pad_right} \\\n",
        "  --checkpoint_path {checkpoint_path} \\\n",
        "  --out_height {out_height} \\\n",
        "  --fullres {resolution_scale} \\\n",
        "  --quality '{quality}' \\\n",
        "  --mask_dilation '{size}' \\\n",
        "  --mask_feathering '{feathering}' \\\n",
        "  --nosmooth '{nosmooth}' \\\n",
        "  --debug_mask '{debug_mask}' \\\n",
        "  --preview_settings '{preview_settings}' \\\n",
        "  --mouth_tracking '{mouth_tracking}'\n",
        "\n",
        "  #end processing timer and format the time it took\n",
        "  end_time = time.time()\n",
        "  elapsed_time = end_time - start_time\n",
        "  process_time = int(elapsed_time)\n",
        "  formatted_process_time = format_time(elapsed_time)\n",
        "\n",
        "  if preview_settings:\n",
        "    if os.path.isfile('temp/preview.jpg'):\n",
        "      clear_output()\n",
        "      display(Image('temp/preview.jpg'))\n",
        "      with open('last_file.txt', 'w') as f:\n",
        "       f.write(temp_input_video)\n",
        "      break\n",
        "    else:\n",
        "      print(f\"Processing failed! :( see line above ðŸ‘†\")\n",
        "      sys.exit(\"Processing failed\")\n",
        "\n",
        "\n",
        "  #rename temp file and move to correct directory\n",
        "  if os.path.isfile(temp_output):\n",
        "    if os.path.isfile(output_video):\n",
        "      os.remove(output_video)\n",
        "    !cp \"{temp_output}\" \"{output_video}\"\n",
        "    #show output video\n",
        "    with open('last_file.txt', 'w') as f:\n",
        "      f.write(temp_input_video)\n",
        "    clear_output()\n",
        "    print(f\"{output_filename} successfully lip synced! Find it in the same folder as your input file(s).\")\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    formatted_setup_time = format_time(elapsed_time)\n",
        "    print(f\"Execution time: {formatted_setup_time}\")\n",
        "    print(f\"Loading video preview for {output_videofile}...\")\n",
        "    show_video(temp_output)\n",
        "    #display(Image(filename='results/p.jpg'))\n",
        "  else:\n",
        "      print(f\"Processing failed! :( see line above ðŸ‘†\")\n",
        "      process_failed = True\n",
        "\n",
        "  if batch_process == False:\n",
        "    #print(\"Batch Processing disabled\")\n",
        "    if process_failed:\n",
        "        sys.exit(\"Processing failed\")\n",
        "    else:\n",
        "      break\n",
        "\n",
        "  elif filenumber == \"\" and audio_filenumber == \"\":\n",
        "    print('Files not set for batch processing')\n",
        "    break\n",
        "\n",
        "  #-----------------------------Batch Processing!------------------------------!\n",
        "  if filenumber != \"\": # if video has a filenumber\n",
        "    match = re.search(r'\\d+', filenumber)\n",
        "    # add 1 to video filenumber\n",
        "    filenumber = f\"{filenumber[:match.start()]}{int(match.group())+1:0{len(match.group())}d}\"\n",
        "\n",
        "  if audio_filenumber != \"\": # if audio has a filenumber\n",
        "    match = re.search(r'\\d+', audio_filenumber)\n",
        "    # add 1 to audio filenumber\n",
        "    audio_filenumber = f\"{audio_filenumber[:match.start()]}{int(match.group())+1:0{len(match.group())}d}\"\n",
        "\n",
        "  # construct input_video\n",
        "  input_video = folder + filenamenonumber + str(filenumber) + file_type\n",
        "  input_videofile = re.search(r\"[^\\/]+$\", input_video).group()\n",
        "  # construct input_audio\n",
        "  input_audio = audio_folder + audio_filenamenonumber + str(audio_filenumber) + audio_file_type\n",
        "  input_audiofile = re.search(r\"[^\\/]+$\", input_audio).group()\n",
        "\n",
        "  # now check which input files exist and what to do for each scenario\n",
        "\n",
        "  # both +1 files exist - continue processing\n",
        "  if os.path.exists(input_video) and os.path.exists(input_audio):\n",
        "    continue\n",
        "\n",
        "  # video +1 only - continue with last audio file\n",
        "  if os.path.exists(input_video) and input_video != last_input_video:\n",
        "    if audio_filenumber != \"\": # if audio has a filenumber\n",
        "        match = re.search(r'\\d+', audio_filenumber)\n",
        "        # take 1 from audio filenumber\n",
        "        audio_filenumber = f\"{audio_filenumber[:match.start()]}{int(match.group())-1:0{len(match.group())}d}\"\n",
        "    continue\n",
        "\n",
        "  # audio +1 only - continue with last video file\n",
        "  if os.path.exists(input_audio) and input_audio != last_input_audio:\n",
        "    if filenumber != \"\": # if video has a filenumber\n",
        "      match = re.search(r'\\d+', filenumber)\n",
        "      # take 1 from video filenumber\n",
        "      filenumber = f\"{filenumber[:match.start()]}{int(match.group())-1:0{len(match.group())}d}\"\n",
        "    continue\n",
        "\n",
        "  # neither +1 files exist or current files already processed - finish processing\n",
        "  print(\"Finished all sequentially numbered files\")\n",
        "  if process_failed:\n",
        "     sys.exit(\"Processing failed on at least one video\")\n",
        "  else:\n",
        "    break"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.10.13 ('Reflexion')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "e475f4c59b316247edcf6b26cbdfcc9d9539991b6afd07b3e79b5b2b94c3b948"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}